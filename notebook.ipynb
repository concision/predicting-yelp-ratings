{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center\">Yelp Rating Prediction</h1>\n",
    "<hr style=\"border-top: 1px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Development Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# python typings\n",
    "from typing import TypedDict, Dict, List\n",
    "import json, time\n",
    "# libraries\n",
    "import sys, numpy, pandas, sklearn, tensorflow\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Numpy {numpy.__version__}\")\n",
    "print(f\"Pandas {pandas.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "print(f\"Tensor Flow Version: {tensorflow.__version__} (Keras Version: {tensorflow.keras.__version__})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]\n",
      "Numpy 1.18.5\n",
      "Pandas 1.1.1\n",
      "Scikit-Learn 0.23.2\n",
      "Tensor Flow Version: 2.3.0 (Keras Version: 2.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Data Importation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Location"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_businesses = r\"data/yelp_academic_dataset_business.json\"\n",
    "file_user_reviews = r\"data/yelp_academic_dataset_review.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Yelp Businesses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 209,393 distinct businesses in 3.433064 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# business structure\n",
    "class Business(TypedDict):\n",
    "    business_id: str\n",
    "    name: str\n",
    "    address: str\n",
    "    city: str\n",
    "    state: str\n",
    "    postal_code: str\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    stars: float\n",
    "    review_count: int\n",
    "    is_open: int\n",
    "    attributes: Dict\n",
    "    categories: List[str]\n",
    "    hours: Dict\n",
    "\n",
    "# businesses indexed by business_id (i.e. {business['business_id']: Business}\n",
    "businesses_by_id: Dict[str, Business] = {}\n",
    "\n",
    "# parse all businesses\n",
    "with open(file_businesses, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        business: Business = json.loads(record)\n",
    "        # map Business by business_id\n",
    "        businesses_by_id[business['business_id']] = business\n",
    "\n",
    "print(f\"Imported {len(businesses_by_id):,} distinct businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import User Review Texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 8,021,122 user reviews in 73.749091 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# user review structure\n",
    "class UserReview(TypedDict):\n",
    "    review_id: str\n",
    "    user_id: str\n",
    "    business_id: str\n",
    "    date: str\n",
    "    stars: int # [0, 1, 2, 3, 4, 5]\n",
    "    text: str\n",
    "    # review ratings\n",
    "    useful: int\n",
    "    funny: int\n",
    "    cool: int\n",
    "\n",
    "# user reviews indexed by business_id (i.e. {business_id: UserReview[]})\n",
    "business_review_texts: Dict[str, List[str]] = {\n",
    "    business_id: [] for business_id in businesses_by_id.keys()\n",
    "}\n",
    "\n",
    "# parse user reviews\n",
    "with open(file_user_reviews, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        review: UserReview = json.loads(record)\n",
    "        # map user review by business_id\n",
    "        business_review_texts[review['business_id']].append(review[\"text\"])\n",
    "\n",
    "print(f\"Imported {sum([len(reviews) for reviews in business_review_texts.values()]):,} user reviews in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Training Data Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select businesses with more than X reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103,803 selected businesses (filtered 105590) in 0.117991 seconds\n"
     ]
    }
   ],
   "source": [
    "# minimum business[\"review_count\"] required for a business to not be filtered\n",
    "MINIMUM_REVIEW_COUNT = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# filter out businesses with less than MINIMUM_REVIEW_COUNT reviews\n",
    "filtered_businesses = [business for business in businesses_by_id.values() if MINIMUM_REVIEW_COUNT <= business[\"review_count\"]]\n",
    "\n",
    "print(f\"{len(filtered_businesses):,} selected businesses (filtered {len(businesses_by_id) - len(filtered_businesses)}) in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partition businesses for model training and testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned 20,000 businesses into {training: 10000, testing: 10000} in 0.109395 seconds\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"represents the absolute number of test samples\"\n",
    "TRAINING_SIZE = 10_000\n",
    "# \"represents the absolute number of train samples\"\n",
    "TESTING_SIZE = 10_000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# split businesses into two disjoint subsets: training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_businesses: List[Business]\n",
    "test_businesses: List[Business]\n",
    "train_businesses, test_businesses = train_test_split(\n",
    "    # set to partition\n",
    "    filtered_businesses,\n",
    "    # partition proportions\n",
    "    train_size = TRAINING_SIZE,\n",
    "    test_size = TESTING_SIZE,\n",
    "    # shuffle the data\n",
    "    shuffle = True,\n",
    "    # PRNG seed for deterministic behaviour\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "print(f\"Partitioned {len(train_businesses) + len(test_businesses):,} businesses into {{training: {len(train_businesses)}, testing: {len(test_businesses)}}} in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# III. Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenate User Reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20,000 businesses in 1.157176 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# concatenated user review texts indexed by business_id (i.e. {business_id: review_text}})\n",
    "business_concatenated_reviews: Dict[str, str] = {}\n",
    "\n",
    "# process each business in the training and testing datasets\n",
    "business: Business\n",
    "for business in train_businesses + test_businesses:\n",
    "    business_id: str = business['business_id']\n",
    "    business_concatenated_reviews[business_id] = \"\\n\".join(business_review_texts[business_id])\n",
    "\n",
    "# metrics\n",
    "print(f\"Processed {len(train_businesses) + len(test_businesses):,} businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vectorize review texts using TFIDF vectorization NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape: (10000, 500)\n",
      "Word features cardinality: 500\n",
      "Word features: ['00', '10', '12', '15', '20', '30', '50', 'able', 'absolutely', 'actually', 'add', 'ago', 'amazing', 'appetizer', 'appointment', 'area', 'arrived', 'ask', 'asked', 'ate', 'atmosphere', 'attentive', 'authentic', 'available', 'average', 'away', 'awesome', 'bacon', 'bad', 'bar', 'bbq', 'beautiful', 'beef', 'beer', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'bowl', 'bread', 'breakfast', 'bring', 'brought', 'brunch', 'buffet', 'burger', 'burgers', 'business', 'busy', 'buy', 'cake', 'called', 'came', 'car', 'card', 'care', 'change', 'charge', 'cheap', 'check', 'cheese', 'chef', 'chicken', 'chips', 'chocolate', 'choice', 'choose', 'clean', 'close', 'coffee', 'cold', 'come', 'comes', 'comfortable', 'coming', 'company', 'completely', 'cooked', 'cool', 'cost', 'couldn', 'counter', 'couple', 'course', 'crab', 'cream', 'crispy', 'curry', 'customer', 'customers', 'cut', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'definitely', 'delicious', 'dessert', 'did', 'didn', 'different', 'dining', 'dinner', 'disappointed', 'dish', 'dishes', 'does', 'doesn', 'dog', 'doing', 'don', 'door', 'dr', 'drink', 'drinks', 'drive', 'dry', 'early', 'easy', 'eat', 'eating', 'egg', 'eggs', 'employees', 'end', 'ended', 'enjoy', 'enjoyed', 'entire', 'especially', 'evening', 'excellent', 'expect', 'expected', 'expensive', 'experience', 'extra', 'extremely', 'fact', 'family', 'fan', 'fantastic', 'far', 'fast', 'favorite', 'feel', 'felt', 'finally', 'fine', 'fish', 'flavor', 'flavors', 'floor', 'food', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'fun', 'garlic', 'gave', 'gets', 'getting', 'girl', 'given', 'glad', 'glass', 'going', 'gone', 'good', 'got', 'great', 'green', 'grilled', 'group', 'guess', 'guy', 'guys', 'hair', 'half', 'happy', 'hard', 'haven', 'having', 'heard', 'help', 'helpful', 'high', 'highly', 'home', 'horrible', 'hot', 'hotel', 'hour', 'hours', 'house', 'huge', 'husband', 'ice', 'immediately', 'impressed', 'inside', 'instead', 'isn', 'issue', 'italian', 'items', 'job', 'just', 'kept', 'kids', 'kind', 'kitchen', 'knew', 'know', 'lady', 'large', 'las', 'late', 'later', 'leave', 'left', 'let', 'life', 'light', 'like', 'liked', 'line', 'list', 'literally', 'little', 'live', 'll', 'lobster', 'local', 'location', 'long', 'look', 'looked', 'looking', 'looks', 'lot', 'lots', 'love', 'loved', 'lunch', 'main', 'make', 'makes', 'making', 'man', 'manager', 'massage', 'maybe', 'meal', 'meat', 'menu', 'mexican', 'milk', 'mind', 'minutes', 'money', 'month', 'months', 'morning', 'moved', 'music', 'nails', 'near', 'need', 'needed', 'needs', 'new', 'nice', 'night', 'noodles', 'offer', 'offered', 'office', 'oh', 'ok', 'okay', 'old', 'open', 'options', 'order', 'ordered', 'ordering', 'orders', 'outside', 'overall', 'owner', 'paid', 'parking', 'party', 'past', 'pasta', 'patio', 'pay', 'people', 'perfect', 'perfectly', 'person', 'phone', 'pick', 'pizza', 'place', 'places', 'plate', 'plenty', 'plus', 'point', 'pool', 'pork', 'portion', 'portions', 'potato', 'potatoes', 'pretty', 'price', 'priced', 'prices', 'probably', 'problem', 'professional', 'quality', 'quick', 'quickly', 'quite', 'ready', 'real', 'really', 'reason', 'reasonable', 'received', 'recommend', 'recommended', 'red', 'regular', 'remember', 'restaurant', 'restaurants', 'return', 'review', 'reviews', 'rice', 'right', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'said', 'salad', 'salmon', 'salsa', 'sandwich', 'sandwiches', 'sat', 'saturday', 'sauce', 'saw', 'say', 'saying', 'seated', 'seating', 'second', 'seen', 'selection', 'serve', 'served', 'server', 'servers', 'service', 'set', 'shop', 'short', 'shrimp', 'simple', 'sit', 'sitting', 'size', 'slow', 'small', 'soft', 'soon', 'soup', 'space', 'special', 'spicy', 'spot', 'staff', 'star', 'stars', 'start', 'started', 'stay', 'steak', 'stop', 'stopped', 'store', 'street', 'strip', 'stuff', 'style', 'sunday', 'super', 'sure', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'taking', 'taste', 'tasted', 'tasty', 'tea', 'tell', 'terrible', 'thai', 'thank', 'thanks', 'thing', 'things', 'think', 'thought', 'time', 'times', 'tip', 'today', 'told', 'took', 'totally', 'town', 'tried', 'trip', 'try', 'trying', 'twice', 'type', 'understand', 'unfortunately', 'use', 'used', 'usually', 'variety', 've', 'vegas', 'visit', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'walking', 'want', 'wanted', 'warm', 'wasn', 'water', 'way', 'week', 'weekend', 'weeks', 'went', 'white', 'wife', 'wine', 'wings', 'wish', 'won', 'wonderful', 'work', 'working', 'worst', 'worth', 'wouldn', 'wow', 'wrong', 'year', 'years', 'yelp', 'yes', 'yummy']\n",
      "IDF Vectorized 20,000 businesses review texts in 109.076089 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# construct vectorizer\n",
    "vectorizer: TfidfVectorizer = TfidfVectorizer(\n",
    "    # maximum word features\n",
    "    max_features = 500,\n",
    "    # prune english stop words\n",
    "    stop_words = 'english',\n",
    "    # min_df: ignore terms that have a document frequency < min_df.\n",
    "    # min_df = 10, (redundant for large data sets)\n",
    ")\n",
    "\n",
    "# construct corpus from training data\n",
    "train_reviews_tfidf = vectorizer.fit_transform(\n",
    "    [business_concatenated_reviews[business['business_id']] for business in train_businesses]\n",
    ").toarray()\n",
    "\n",
    "# transform corpus of testing data\n",
    "test_reviews_tfidf = vectorizer.transform(\n",
    "    [business_concatenated_reviews[business['business_id']] for business in test_businesses]\n",
    ").toarray()\n",
    "\n",
    "# metrics\n",
    "print(f\"Training matrix shape: {train_reviews_tfidf.shape}\")\n",
    "print(f\"Word features cardinality: {len(vectorizer.get_feature_names()):,}\")\n",
    "print(f\"Word features: {vectorizer.get_feature_names()}\")\n",
    "print(f\"IDF Vectorized {len(train_businesses) + len(test_businesses):,} businesses review texts in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Input Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tFeatures shape: (10000, 500)\n",
      "Testing:\n",
      "\tFeatures shape: (10000, 500)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_tf_input = train_reviews_tfidf\n",
    "print(\"Training:\")\n",
    "print(f\"\\tFeatures shape: {train_tf_input.shape}\")\n",
    "\n",
    "# testing\n",
    "test_tf_input = test_reviews_tfidf\n",
    "print(\"Testing:\")\n",
    "print(f\"\\tFeatures shape: {test_tf_input.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Star Rating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_stars = numpy.array([[business['stars']] for business in train_businesses])\n",
    "test_stars = numpy.array([[business[\"stars\"]] for business in test_businesses])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tFeatures shape: (10000, 1)\n",
      "Testing:\n",
      "\tFeatures shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_tf_output = train_stars\n",
    "print(\"Training:\")\n",
    "print(f\"\\tFeatures shape: {train_tf_output.shape}\")\n",
    "\n",
    "# testing\n",
    "test_tf_output = test_stars\n",
    "print(\"Testing:\")\n",
    "print(f\"\\tFeatures shape: {test_tf_output.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Training\n",
    "\n",
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "313/313 - 1s - loss: 0.9184 - mse: 0.9184 - val_loss: 0.4109 - val_mse: 0.4109\n",
      "Epoch 2/250\n",
      "313/313 - 0s - loss: 0.2919 - mse: 0.2919 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 3/250\n",
      "313/313 - 1s - loss: 0.2063 - mse: 0.2063 - val_loss: 0.1979 - val_mse: 0.1979\n",
      "Epoch 4/250\n",
      "313/313 - 1s - loss: 0.1883 - mse: 0.1883 - val_loss: 0.1900 - val_mse: 0.1900\n",
      "Epoch 5/250\n",
      "313/313 - 1s - loss: 0.1787 - mse: 0.1787 - val_loss: 0.1731 - val_mse: 0.1731\n",
      "Epoch 6/250\n",
      "313/313 - 1s - loss: 0.1717 - mse: 0.1717 - val_loss: 0.1735 - val_mse: 0.1735\n",
      "Epoch 7/250\n",
      "313/313 - 1s - loss: 0.1658 - mse: 0.1658 - val_loss: 0.1781 - val_mse: 0.1781\n",
      "Epoch 8/250\n",
      "313/313 - 1s - loss: 0.1615 - mse: 0.1615 - val_loss: 0.1604 - val_mse: 0.1604\n",
      "Epoch 9/250\n",
      "313/313 - 1s - loss: 0.1581 - mse: 0.1581 - val_loss: 0.1575 - val_mse: 0.1575\n",
      "Epoch 10/250\n",
      "313/313 - 1s - loss: 0.1553 - mse: 0.1553 - val_loss: 0.1569 - val_mse: 0.1569\n",
      "Epoch 11/250\n",
      "313/313 - 0s - loss: 0.1524 - mse: 0.1524 - val_loss: 0.1535 - val_mse: 0.1535\n",
      "Epoch 12/250\n",
      "313/313 - 1s - loss: 0.1504 - mse: 0.1504 - val_loss: 0.1521 - val_mse: 0.1521\n",
      "Epoch 13/250\n",
      "313/313 - 1s - loss: 0.1475 - mse: 0.1475 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 14/250\n",
      "313/313 - 1s - loss: 0.1469 - mse: 0.1469 - val_loss: 0.1621 - val_mse: 0.1621\n",
      "Epoch 15/250\n",
      "313/313 - 1s - loss: 0.1449 - mse: 0.1449 - val_loss: 0.1709 - val_mse: 0.1709\n",
      "Epoch 16/250\n",
      "313/313 - 0s - loss: 0.1434 - mse: 0.1434 - val_loss: 0.1491 - val_mse: 0.1491\n",
      "Epoch 17/250\n",
      "313/313 - 1s - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 18/250\n",
      "313/313 - 1s - loss: 0.1409 - mse: 0.1409 - val_loss: 0.1704 - val_mse: 0.1704\n",
      "Epoch 19/250\n",
      "313/313 - 1s - loss: 0.1395 - mse: 0.1395 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 20/250\n",
      "313/313 - 1s - loss: 0.1390 - mse: 0.1390 - val_loss: 0.1429 - val_mse: 0.1429\n",
      "Epoch 21/250\n",
      "313/313 - 0s - loss: 0.1371 - mse: 0.1371 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 22/250\n",
      "313/313 - 0s - loss: 0.1369 - mse: 0.1369 - val_loss: 0.1446 - val_mse: 0.1446\n",
      "Epoch 23/250\n",
      "313/313 - 0s - loss: 0.1356 - mse: 0.1356 - val_loss: 0.1513 - val_mse: 0.1513\n",
      "Epoch 24/250\n",
      "313/313 - 1s - loss: 0.1353 - mse: 0.1353 - val_loss: 0.1410 - val_mse: 0.1410\n",
      "Epoch 25/250\n",
      "313/313 - 0s - loss: 0.1333 - mse: 0.1333 - val_loss: 0.1445 - val_mse: 0.1445\n",
      "Epoch 26/250\n",
      "313/313 - 1s - loss: 0.1326 - mse: 0.1326 - val_loss: 0.1386 - val_mse: 0.1386\n",
      "Epoch 27/250\n",
      "313/313 - 1s - loss: 0.1320 - mse: 0.1320 - val_loss: 0.1384 - val_mse: 0.1384\n",
      "Epoch 28/250\n",
      "313/313 - 1s - loss: 0.1315 - mse: 0.1315 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 29/250\n",
      "313/313 - 0s - loss: 0.1303 - mse: 0.1303 - val_loss: 0.1547 - val_mse: 0.1547\n",
      "Epoch 30/250\n",
      "313/313 - 1s - loss: 0.1294 - mse: 0.1294 - val_loss: 0.1394 - val_mse: 0.1394\n",
      "Epoch 31/250\n",
      "313/313 - 0s - loss: 0.1288 - mse: 0.1288 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 32/250\n",
      "313/313 - 1s - loss: 0.1281 - mse: 0.1281 - val_loss: 0.1438 - val_mse: 0.1438\n",
      "Epoch 33/250\n",
      "313/313 - 1s - loss: 0.1274 - mse: 0.1274 - val_loss: 0.1391 - val_mse: 0.1391\n",
      "Epoch 34/250\n",
      "313/313 - 1s - loss: 0.1266 - mse: 0.1266 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 35/250\n",
      "313/313 - 1s - loss: 0.1259 - mse: 0.1259 - val_loss: 0.1425 - val_mse: 0.1425\n",
      "Epoch 36/250\n",
      "313/313 - 1s - loss: 0.1253 - mse: 0.1253 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 37/250\n",
      "313/313 - 0s - loss: 0.1246 - mse: 0.1246 - val_loss: 0.1345 - val_mse: 0.1345\n",
      "Epoch 38/250\n",
      "313/313 - 1s - loss: 0.1238 - mse: 0.1238 - val_loss: 0.1320 - val_mse: 0.1320\n",
      "Epoch 39/250\n",
      "313/313 - 1s - loss: 0.1230 - mse: 0.1230 - val_loss: 0.1352 - val_mse: 0.1352\n",
      "Epoch 40/250\n",
      "313/313 - 0s - loss: 0.1231 - mse: 0.1231 - val_loss: 0.1311 - val_mse: 0.1311\n",
      "Epoch 41/250\n",
      "313/313 - 0s - loss: 0.1217 - mse: 0.1217 - val_loss: 0.1309 - val_mse: 0.1309\n",
      "Epoch 42/250\n",
      "313/313 - 1s - loss: 0.1215 - mse: 0.1215 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 43/250\n",
      "313/313 - 0s - loss: 0.1207 - mse: 0.1207 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 44/250\n",
      "313/313 - 1s - loss: 0.1204 - mse: 0.1204 - val_loss: 0.1302 - val_mse: 0.1302\n",
      "Epoch 45/250\n",
      "313/313 - 0s - loss: 0.1195 - mse: 0.1195 - val_loss: 0.1297 - val_mse: 0.1297\n",
      "Epoch 46/250\n",
      "313/313 - 1s - loss: 0.1193 - mse: 0.1193 - val_loss: 0.1320 - val_mse: 0.1320\n",
      "Epoch 47/250\n",
      "313/313 - 0s - loss: 0.1192 - mse: 0.1192 - val_loss: 0.1281 - val_mse: 0.1281\n",
      "Epoch 48/250\n",
      "313/313 - 0s - loss: 0.1191 - mse: 0.1191 - val_loss: 0.1281 - val_mse: 0.1281\n",
      "Epoch 49/250\n",
      "313/313 - 0s - loss: 0.1184 - mse: 0.1184 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 50/250\n",
      "313/313 - 1s - loss: 0.1177 - mse: 0.1177 - val_loss: 0.1501 - val_mse: 0.1501\n",
      "Epoch 51/250\n",
      "313/313 - 1s - loss: 0.1169 - mse: 0.1169 - val_loss: 0.1273 - val_mse: 0.1273\n",
      "Epoch 52/250\n",
      "313/313 - 1s - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1346 - val_mse: 0.1346\n",
      "Epoch 53/250\n",
      "313/313 - 1s - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1420 - val_mse: 0.1420\n",
      "Epoch 54/250\n",
      "313/313 - 0s - loss: 0.1159 - mse: 0.1159 - val_loss: 0.1264 - val_mse: 0.1264\n",
      "Epoch 55/250\n",
      "313/313 - 1s - loss: 0.1158 - mse: 0.1158 - val_loss: 0.1262 - val_mse: 0.1262\n",
      "Epoch 56/250\n",
      "313/313 - 0s - loss: 0.1147 - mse: 0.1147 - val_loss: 0.1263 - val_mse: 0.1263\n",
      "Epoch 57/250\n",
      "313/313 - 1s - loss: 0.1145 - mse: 0.1145 - val_loss: 0.1300 - val_mse: 0.1300\n",
      "Epoch 58/250\n",
      "313/313 - 1s - loss: 0.1140 - mse: 0.1140 - val_loss: 0.1286 - val_mse: 0.1286\n",
      "Epoch 59/250\n",
      "313/313 - 0s - loss: 0.1137 - mse: 0.1137 - val_loss: 0.1258 - val_mse: 0.1258\n",
      "Epoch 60/250\n",
      "313/313 - 0s - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1268 - val_mse: 0.1268\n",
      "Epoch 61/250\n",
      "313/313 - 1s - loss: 0.1135 - mse: 0.1135 - val_loss: 0.1250 - val_mse: 0.1250\n",
      "Epoch 62/250\n",
      "313/313 - 0s - loss: 0.1126 - mse: 0.1126 - val_loss: 0.1303 - val_mse: 0.1303\n",
      "Epoch 63/250\n",
      "313/313 - 0s - loss: 0.1122 - mse: 0.1122 - val_loss: 0.1264 - val_mse: 0.1264\n",
      "Epoch 64/250\n",
      "313/313 - 1s - loss: 0.1127 - mse: 0.1127 - val_loss: 0.1246 - val_mse: 0.1246\n",
      "Epoch 65/250\n",
      "313/313 - 1s - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1249 - val_mse: 0.1249\n",
      "Epoch 66/250\n",
      "313/313 - 0s - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1272 - val_mse: 0.1272\n",
      "Epoch 67/250\n",
      "313/313 - 1s - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 68/250\n",
      "313/313 - 0s - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1269 - val_mse: 0.1269\n",
      "Epoch 69/250\n",
      "313/313 - 0s - loss: 0.1109 - mse: 0.1109 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 70/250\n",
      "313/313 - 0s - loss: 0.1107 - mse: 0.1107 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 71/250\n",
      "313/313 - 0s - loss: 0.1102 - mse: 0.1102 - val_loss: 0.1301 - val_mse: 0.1301\n",
      "Epoch 72/250\n",
      "313/313 - 0s - loss: 0.1100 - mse: 0.1100 - val_loss: 0.1231 - val_mse: 0.1231\n",
      "Epoch 73/250\n",
      "313/313 - 1s - loss: 0.1097 - mse: 0.1097 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 74/250\n",
      "313/313 - 1s - loss: 0.1094 - mse: 0.1094 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 75/250\n",
      "313/313 - 1s - loss: 0.1087 - mse: 0.1087 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 76/250\n",
      "313/313 - 0s - loss: 0.1087 - mse: 0.1087 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 77/250\n",
      "313/313 - 1s - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1231 - val_mse: 0.1231\n",
      "Epoch 78/250\n",
      "313/313 - 1s - loss: 0.1083 - mse: 0.1083 - val_loss: 0.1222 - val_mse: 0.1222\n",
      "Epoch 79/250\n",
      "313/313 - 0s - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 80/250\n",
      "313/313 - 0s - loss: 0.1076 - mse: 0.1076 - val_loss: 0.1253 - val_mse: 0.1253\n",
      "Epoch 81/250\n",
      "313/313 - 1s - loss: 0.1078 - mse: 0.1078 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 82/250\n",
      "313/313 - 1s - loss: 0.1067 - mse: 0.1067 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 83/250\n",
      "313/313 - 1s - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1249 - val_mse: 0.1249\n",
      "Epoch 84/250\n",
      "313/313 - 0s - loss: 0.1066 - mse: 0.1066 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 85/250\n",
      "313/313 - 0s - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 86/250\n",
      "313/313 - 0s - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1222 - val_mse: 0.1222\n",
      "Epoch 87/250\n",
      "313/313 - 0s - loss: 0.1056 - mse: 0.1056 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 88/250\n",
      "313/313 - 1s - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1219 - val_mse: 0.1219\n",
      "Epoch 89/250\n",
      "313/313 - 0s - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 90/250\n",
      "313/313 - 0s - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1221 - val_mse: 0.1221\n",
      "Epoch 91/250\n",
      "313/313 - 0s - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1201 - val_mse: 0.1201\n",
      "Epoch 92/250\n",
      "313/313 - 0s - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1267 - val_mse: 0.1267\n",
      "Epoch 93/250\n",
      "313/313 - 0s - loss: 0.1046 - mse: 0.1046 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 94/250\n",
      "313/313 - 1s - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1242 - val_mse: 0.1242\n",
      "Epoch 95/250\n",
      "313/313 - 1s - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 96/250\n",
      "313/313 - 0s - loss: 0.1037 - mse: 0.1037 - val_loss: 0.1197 - val_mse: 0.1197\n",
      "Epoch 97/250\n",
      "313/313 - 1s - loss: 0.1036 - mse: 0.1036 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 98/250\n",
      "313/313 - 1s - loss: 0.1037 - mse: 0.1037 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 99/250\n",
      "313/313 - 1s - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1195 - val_mse: 0.1195\n",
      "Epoch 100/250\n",
      "313/313 - 0s - loss: 0.1032 - mse: 0.1032 - val_loss: 0.1197 - val_mse: 0.1197\n",
      "Epoch 101/250\n",
      "313/313 - 1s - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1199 - val_mse: 0.1199\n",
      "Epoch 102/250\n",
      "313/313 - 1s - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 103/250\n",
      "313/313 - 0s - loss: 0.1025 - mse: 0.1025 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 104/250\n",
      "313/313 - 0s - loss: 0.1023 - mse: 0.1023 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 105/250\n",
      "313/313 - 1s - loss: 0.1022 - mse: 0.1022 - val_loss: 0.1199 - val_mse: 0.1199\n",
      "Epoch 106/250\n",
      "313/313 - 1s - loss: 0.1018 - mse: 0.1018 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 107/250\n",
      "313/313 - 0s - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1211 - val_mse: 0.1211\n",
      "Epoch 108/250\n",
      "313/313 - 1s - loss: 0.1016 - mse: 0.1016 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 109/250\n",
      "313/313 - 1s - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1198 - val_mse: 0.1198\n",
      "Epoch 110/250\n",
      "313/313 - 1s - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1202 - val_mse: 0.1202\n",
      "Epoch 111/250\n",
      "313/313 - 1s - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1201 - val_mse: 0.1201\n",
      "Epoch 112/250\n",
      "313/313 - 0s - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1185 - val_mse: 0.1185\n",
      "Epoch 113/250\n",
      "313/313 - 1s - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1184 - val_mse: 0.1184\n",
      "Epoch 114/250\n",
      "313/313 - 0s - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1207 - val_mse: 0.1207\n",
      "Epoch 115/250\n",
      "313/313 - 0s - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1187 - val_mse: 0.1187\n",
      "Epoch 116/250\n",
      "313/313 - 1s - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 117/250\n",
      "313/313 - 0s - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1264 - val_mse: 0.1264\n",
      "Epoch 118/250\n",
      "313/313 - 1s - loss: 0.0996 - mse: 0.0996 - val_loss: 0.1276 - val_mse: 0.1276\n",
      "Epoch 119/250\n",
      "313/313 - 1s - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1201 - val_mse: 0.1201\n",
      "Epoch 120/250\n",
      "313/313 - 0s - loss: 0.0994 - mse: 0.0994 - val_loss: 0.1257 - val_mse: 0.1257\n",
      "Epoch 121/250\n",
      "313/313 - 1s - loss: 0.0993 - mse: 0.0993 - val_loss: 0.1217 - val_mse: 0.1217\n",
      "Epoch 122/250\n",
      "313/313 - 1s - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1178 - val_mse: 0.1178\n",
      "Epoch 123/250\n",
      "313/313 - 1s - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1193 - val_mse: 0.1193\n",
      "Epoch 124/250\n",
      "313/313 - 0s - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1197 - val_mse: 0.1197\n",
      "Epoch 125/250\n",
      "313/313 - 1s - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1225 - val_mse: 0.1225\n",
      "Epoch 126/250\n",
      "313/313 - 1s - loss: 0.0982 - mse: 0.0982 - val_loss: 0.1179 - val_mse: 0.1179\n",
      "Epoch 127/250\n",
      "313/313 - 1s - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 128/250\n",
      "313/313 - 0s - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1185 - val_mse: 0.1185\n",
      "Epoch 129/250\n",
      "313/313 - 0s - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1265 - val_mse: 0.1265\n",
      "Epoch 130/250\n",
      "313/313 - 1s - loss: 0.0980 - mse: 0.0980 - val_loss: 0.1190 - val_mse: 0.1190\n",
      "Epoch 131/250\n",
      "313/313 - 0s - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1308 - val_mse: 0.1308\n",
      "Epoch 132/250\n",
      "313/313 - 0s - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 133/250\n",
      "313/313 - 1s - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 134/250\n",
      "313/313 - 1s - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1173 - val_mse: 0.1173\n",
      "Epoch 135/250\n",
      "313/313 - 1s - loss: 0.0969 - mse: 0.0969 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 136/250\n",
      "313/313 - 1s - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 137/250\n",
      "313/313 - 0s - loss: 0.0965 - mse: 0.0965 - val_loss: 0.1172 - val_mse: 0.1172\n",
      "Epoch 138/250\n",
      "313/313 - 0s - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1206 - val_mse: 0.1206\n",
      "Epoch 139/250\n",
      "313/313 - 0s - loss: 0.0969 - mse: 0.0969 - val_loss: 0.1175 - val_mse: 0.1175\n",
      "Epoch 140/250\n",
      "313/313 - 1s - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1288 - val_mse: 0.1288\n",
      "Epoch 141/250\n",
      "313/313 - 1s - loss: 0.0959 - mse: 0.0959 - val_loss: 0.1193 - val_mse: 0.1193\n",
      "Model trained in 75.569601 seconds\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "# add new neurons\n",
    "model.add(Dense(50, input_dim = train_tf_input.shape[1]))\n",
    "model.add(Dense(25, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "# set optimizer for gradient descent\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['mse'])\n",
    "\n",
    "model.fit(\n",
    "    # training data\n",
    "    train_tf_input, train_tf_output,\n",
    "    # use test data to validate losses, but not for training\n",
    "    validation_data = (test_tf_input, test_tf_output),\n",
    "    callbacks = [\n",
    "        # patience: number of epochs with no improvement after which training will be stopped\n",
    "        EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 25, mode = 'auto', verbose = 0),\n",
    "        # save best model from all trials\n",
    "        ModelCheckpoint(filepath = \"temp/model_best_weights.hdf5\", save_best_only = True, verbose = 0)\n",
    "    ],\n",
    "    verbose = 2,\n",
    "    epochs = 250\n",
    ")\n",
    "\n",
    "print(f\"Model trained in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}