{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center\">Yelp Rating Prediction</h1>\n",
    "<hr style=\"border-top: 1px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Development Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# python typings\n",
    "from typing import TypedDict, Dict, List\n",
    "import json, time\n",
    "# libraries\n",
    "import sys, numpy, pandas, sklearn, tensorflow\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Numpy {numpy.__version__}\")\n",
    "print(f\"Pandas {pandas.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "print(f\"Tensor Flow Version: {tensorflow.__version__} (Keras Version: {tensorflow.keras.__version__})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]\n",
      "Numpy 1.18.5\n",
      "Pandas 1.1.1\n",
      "Scikit-Learn 0.23.2\n",
      "Tensor Flow Version: 2.3.0 (Keras Version: 2.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Data Importation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Location"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_businesses = r\"data/yelp_academic_dataset_business.json\"\n",
    "file_user_reviews = r\"data/yelp_academic_dataset_review.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Yelp Businesses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 209,393 distinct businesses in 3.457917 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# business structure\n",
    "class Business(TypedDict):\n",
    "    business_id: str\n",
    "    name: str\n",
    "    address: str\n",
    "    city: str\n",
    "    state: str\n",
    "    postal_code: str\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    stars: float\n",
    "    review_count: int\n",
    "    is_open: int\n",
    "    attributes: Dict\n",
    "    categories: List[str]\n",
    "    hours: Dict\n",
    "\n",
    "# businesses indexed by business_id (i.e. {business['business_id']: Business}\n",
    "businesses_by_id: Dict[str, Business] = {}\n",
    "\n",
    "# parse all businesses\n",
    "with open(file_businesses, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        business: Business = json.loads(record)\n",
    "        # map Business by business_id\n",
    "        businesses_by_id[business['business_id']] = business\n",
    "\n",
    "print(f\"Imported {len(businesses_by_id):,} distinct businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import User Review Texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 8,021,122 user reviews in 81.313607 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# user review structure\n",
    "class UserReview(TypedDict):\n",
    "    review_id: str\n",
    "    user_id: str\n",
    "    business_id: str\n",
    "    date: str\n",
    "    stars: int # [0, 1, 2, 3, 4, 5]\n",
    "    text: str\n",
    "    # review ratings\n",
    "    useful: int\n",
    "    funny: int\n",
    "    cool: int\n",
    "\n",
    "# user reviews indexed by business_id (i.e. {business_id: UserReview[]})\n",
    "business_review_texts: Dict[str, List[str]] = {\n",
    "    business_id: [] for business_id in businesses_by_id.keys()\n",
    "}\n",
    "\n",
    "# parse user reviews\n",
    "with open(file_user_reviews, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        review: UserReview = json.loads(record)\n",
    "        # map user review by business_id\n",
    "        business_review_texts[review['business_id']].append(review[\"text\"])\n",
    "\n",
    "print(f\"Imported {sum([len(reviews) for reviews in business_review_texts.values()]):,} user reviews in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Training Data Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select businesses with more than X reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103,803 selected businesses (filtered 105590) in 0.133837 seconds\n"
     ]
    }
   ],
   "source": [
    "# minimum business[\"review_count\"] required for a business to not be filtered\n",
    "MINIMUM_REVIEW_COUNT = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# filter out businesses with less than MINIMUM_REVIEW_COUNT reviews\n",
    "filtered_businesses = [business for business in businesses_by_id.values() if MINIMUM_REVIEW_COUNT <= business[\"review_count\"]]\n",
    "\n",
    "print(f\"{len(filtered_businesses):,} selected businesses (filtered {len(businesses_by_id) - len(filtered_businesses)}) in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partition businesses for model training and testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned 20,000 businesses into {training: 10000, testing: 10000} in 0.131517 seconds\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"represents the absolute number of test samples\"\n",
    "TRAINING_SIZE = 10_000\n",
    "# \"represents the absolute number of train samples\"\n",
    "TESTING_SIZE = 10_000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# split businesses into two disjoint subsets: training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_businesses: List[Business]\n",
    "test_businesses: List[Business]\n",
    "train_businesses, test_businesses = train_test_split(\n",
    "    # set to partition\n",
    "    filtered_businesses,\n",
    "    # partition proportions\n",
    "    train_size = TRAINING_SIZE,\n",
    "    test_size = TESTING_SIZE,\n",
    "    # shuffle the data\n",
    "    shuffle = True,\n",
    "    # PRNG seed for deterministic behaviour\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "print(f\"Partitioned {len(train_businesses) + len(test_businesses):,} businesses into {{training: {len(train_businesses)}, testing: {len(test_businesses)}}} in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# III. Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenate User Reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20,000 businesses in 1.311225 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# concatenated user review texts indexed by business_id (i.e. {business_id: review_text}})\n",
    "business_concatenated_reviews: Dict[str, str] = {}\n",
    "\n",
    "# process each business in the training and testing datasets\n",
    "business: Business\n",
    "for business in train_businesses + test_businesses:\n",
    "    business_id: str = business['business_id']\n",
    "    business_concatenated_reviews[business_id] = \"\\n\".join(business_review_texts[business_id])\n",
    "\n",
    "# metrics\n",
    "print(f\"Processed {len(train_businesses) + len(test_businesses):,} businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vectorize review texts using TFIDF vectorization NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape: (10000, 500)\n",
      "Word features cardinality: 500\n",
      "Word features: ['00', '10', '12', '15', '20', '30', '50', 'able', 'absolutely', 'actually', 'add', 'ago', 'amazing', 'appetizer', 'appointment', 'area', 'arrived', 'ask', 'asked', 'ate', 'atmosphere', 'attentive', 'authentic', 'available', 'average', 'away', 'awesome', 'bacon', 'bad', 'bar', 'bbq', 'beautiful', 'beef', 'beer', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'bowl', 'bread', 'breakfast', 'bring', 'brought', 'brunch', 'buffet', 'burger', 'burgers', 'business', 'busy', 'buy', 'cake', 'called', 'came', 'car', 'card', 'care', 'change', 'charge', 'cheap', 'check', 'cheese', 'chef', 'chicken', 'chips', 'chocolate', 'choice', 'choose', 'clean', 'close', 'coffee', 'cold', 'come', 'comes', 'comfortable', 'coming', 'company', 'completely', 'cooked', 'cool', 'cost', 'couldn', 'counter', 'couple', 'course', 'crab', 'cream', 'crispy', 'curry', 'customer', 'customers', 'cut', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'definitely', 'delicious', 'dessert', 'did', 'didn', 'different', 'dining', 'dinner', 'disappointed', 'dish', 'dishes', 'does', 'doesn', 'dog', 'doing', 'don', 'door', 'dr', 'drink', 'drinks', 'drive', 'dry', 'early', 'easy', 'eat', 'eating', 'egg', 'eggs', 'employees', 'end', 'ended', 'enjoy', 'enjoyed', 'entire', 'especially', 'evening', 'excellent', 'expect', 'expected', 'expensive', 'experience', 'extra', 'extremely', 'fact', 'family', 'fan', 'fantastic', 'far', 'fast', 'favorite', 'feel', 'felt', 'finally', 'fine', 'fish', 'flavor', 'flavors', 'floor', 'food', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'fun', 'garlic', 'gave', 'gets', 'getting', 'girl', 'given', 'glad', 'glass', 'going', 'gone', 'good', 'got', 'great', 'green', 'grilled', 'group', 'guess', 'guy', 'guys', 'hair', 'half', 'happy', 'hard', 'haven', 'having', 'heard', 'help', 'helpful', 'high', 'highly', 'home', 'horrible', 'hot', 'hotel', 'hour', 'hours', 'house', 'huge', 'husband', 'ice', 'immediately', 'impressed', 'inside', 'instead', 'isn', 'issue', 'italian', 'items', 'job', 'just', 'kept', 'kids', 'kind', 'kitchen', 'knew', 'know', 'lady', 'large', 'las', 'late', 'later', 'leave', 'left', 'let', 'life', 'light', 'like', 'liked', 'line', 'list', 'literally', 'little', 'live', 'll', 'lobster', 'local', 'location', 'long', 'look', 'looked', 'looking', 'looks', 'lot', 'lots', 'love', 'loved', 'lunch', 'main', 'make', 'makes', 'making', 'man', 'manager', 'massage', 'maybe', 'meal', 'meat', 'menu', 'mexican', 'milk', 'mind', 'minutes', 'money', 'month', 'months', 'morning', 'moved', 'music', 'nails', 'near', 'need', 'needed', 'needs', 'new', 'nice', 'night', 'noodles', 'offer', 'offered', 'office', 'oh', 'ok', 'okay', 'old', 'open', 'options', 'order', 'ordered', 'ordering', 'orders', 'outside', 'overall', 'owner', 'paid', 'parking', 'party', 'past', 'pasta', 'patio', 'pay', 'people', 'perfect', 'perfectly', 'person', 'phone', 'pick', 'pizza', 'place', 'places', 'plate', 'plenty', 'plus', 'point', 'pool', 'pork', 'portion', 'portions', 'potato', 'potatoes', 'pretty', 'price', 'priced', 'prices', 'probably', 'problem', 'professional', 'quality', 'quick', 'quickly', 'quite', 'ready', 'real', 'really', 'reason', 'reasonable', 'received', 'recommend', 'recommended', 'red', 'regular', 'remember', 'restaurant', 'restaurants', 'return', 'review', 'reviews', 'rice', 'right', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'said', 'salad', 'salmon', 'salsa', 'sandwich', 'sandwiches', 'sat', 'saturday', 'sauce', 'saw', 'say', 'saying', 'seated', 'seating', 'second', 'seen', 'selection', 'serve', 'served', 'server', 'servers', 'service', 'set', 'shop', 'short', 'shrimp', 'simple', 'sit', 'sitting', 'size', 'slow', 'small', 'soft', 'soon', 'soup', 'space', 'special', 'spicy', 'spot', 'staff', 'star', 'stars', 'start', 'started', 'stay', 'steak', 'stop', 'stopped', 'store', 'street', 'strip', 'stuff', 'style', 'sunday', 'super', 'sure', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'taking', 'taste', 'tasted', 'tasty', 'tea', 'tell', 'terrible', 'thai', 'thank', 'thanks', 'thing', 'things', 'think', 'thought', 'time', 'times', 'tip', 'today', 'told', 'took', 'totally', 'town', 'tried', 'trip', 'try', 'trying', 'twice', 'type', 'understand', 'unfortunately', 'use', 'used', 'usually', 'variety', 've', 'vegas', 'visit', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'walking', 'want', 'wanted', 'warm', 'wasn', 'water', 'way', 'week', 'weekend', 'weeks', 'went', 'white', 'wife', 'wine', 'wings', 'wish', 'won', 'wonderful', 'work', 'working', 'worst', 'worth', 'wouldn', 'wow', 'wrong', 'year', 'years', 'yelp', 'yes', 'yummy']\n",
      "IDF Vectorized 20,000 businesses review texts in 193.062387 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# construct vectorizer\n",
    "vectorizer: TfidfVectorizer = TfidfVectorizer(\n",
    "    # maximum word features\n",
    "    max_features = 500,\n",
    "    # prune english stop words\n",
    "    stop_words = 'english',\n",
    "    # min_df: ignore terms that have a document frequency < min_df.\n",
    "    # min_df = 10, (redundant for large data sets)\n",
    ")\n",
    "\n",
    "# construct corpus from training data\n",
    "train_reviews_tfidf = vectorizer.fit_transform(\n",
    "    [business_concatenated_reviews[business['business_id']] for business in train_businesses]\n",
    ").toarray()\n",
    "\n",
    "# transform corpus of testing data\n",
    "test_reviews_tfidf = vectorizer.transform(\n",
    "    [business_concatenated_reviews[business['business_id']] for business in test_businesses]\n",
    ").toarray()\n",
    "\n",
    "# metrics\n",
    "print(f\"Training matrix shape: {train_reviews_tfidf.shape}\")\n",
    "print(f\"Word features cardinality: {len(vectorizer.get_feature_names()):,}\")\n",
    "print(f\"Word features: {vectorizer.get_feature_names()}\")\n",
    "print(f\"IDF Vectorized {len(train_businesses) + len(test_businesses):,} businesses review texts in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Input Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tFeatures shape: (10000, 500)\n",
      "Testing:\n",
      "\tFeatures shape: (10000, 500)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_tf_input = train_reviews_tfidf\n",
    "print(\"Training:\")\n",
    "print(f\"\\tFeatures shape: {train_tf_input.shape}\")\n",
    "\n",
    "# testing\n",
    "test_tf_input = test_reviews_tfidf\n",
    "print(\"Testing:\")\n",
    "print(f\"\\tFeatures shape: {test_tf_input.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Star Rating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_stars = numpy.array([[business['stars']] for business in train_businesses])\n",
    "test_stars = numpy.array([[business[\"stars\"]] for business in test_businesses])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tFeatures shape: (10000, 1)\n",
      "Testing:\n",
      "\tFeatures shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_tf_output = train_stars\n",
    "print(\"Training:\")\n",
    "print(f\"\\tFeatures shape: {train_tf_output.shape}\")\n",
    "\n",
    "# testing\n",
    "test_tf_output = test_stars\n",
    "print(\"Testing:\")\n",
    "print(f\"\\tFeatures shape: {test_tf_output.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 64.926820 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "    <details>        <summary><b>Training Details</b></summary>        <sub><sup>            Epoch 1/250<br>313/313 - 1s - loss: 0.8723 - mse: 0.8723 - val_loss: 0.4026 - val_mse: 0.4026<br>Epoch 2/250<br>313/313 - 1s - loss: 0.2907 - mse: 0.2907 - val_loss: 0.2477 - val_mse: 0.2477<br>Epoch 3/250<br>313/313 - 1s - loss: 0.2127 - mse: 0.2127 - val_loss: 0.1997 - val_mse: 0.1997<br>Epoch 4/250<br>313/313 - 1s - loss: 0.1947 - mse: 0.1947 - val_loss: 0.1932 - val_mse: 0.1932<br>Epoch 5/250<br>313/313 - 1s - loss: 0.1829 - mse: 0.1829 - val_loss: 0.1832 - val_mse: 0.1832<br>Epoch 6/250<br>313/313 - 1s - loss: 0.1750 - mse: 0.1750 - val_loss: 0.1837 - val_mse: 0.1837<br>Epoch 7/250<br>313/313 - 1s - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1690 - val_mse: 0.1690<br>Epoch 8/250<br>313/313 - 0s - loss: 0.1638 - mse: 0.1638 - val_loss: 0.1668 - val_mse: 0.1668<br>Epoch 9/250<br>313/313 - 1s - loss: 0.1600 - mse: 0.1600 - val_loss: 0.1610 - val_mse: 0.1610<br>Epoch 10/250<br>313/313 - 1s - loss: 0.1568 - mse: 0.1568 - val_loss: 0.1594 - val_mse: 0.1594<br>Epoch 11/250<br>313/313 - 1s - loss: 0.1539 - mse: 0.1539 - val_loss: 0.1640 - val_mse: 0.1640<br>Epoch 12/250<br>313/313 - 1s - loss: 0.1518 - mse: 0.1518 - val_loss: 0.1548 - val_mse: 0.1548<br>Epoch 13/250<br>313/313 - 1s - loss: 0.1487 - mse: 0.1487 - val_loss: 0.1587 - val_mse: 0.1587<br>Epoch 14/250<br>313/313 - 1s - loss: 0.1469 - mse: 0.1469 - val_loss: 0.1572 - val_mse: 0.1572<br>Epoch 15/250<br>313/313 - 1s - loss: 0.1459 - mse: 0.1459 - val_loss: 0.1623 - val_mse: 0.1623<br>Epoch 16/250<br>313/313 - 0s - loss: 0.1439 - mse: 0.1439 - val_loss: 0.1641 - val_mse: 0.1641<br>Epoch 17/250<br>313/313 - 1s - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1558 - val_mse: 0.1558<br>Epoch 18/250<br>313/313 - 1s - loss: 0.1420 - mse: 0.1420 - val_loss: 0.1471 - val_mse: 0.1471<br>Epoch 19/250<br>313/313 - 1s - loss: 0.1399 - mse: 0.1399 - val_loss: 0.1474 - val_mse: 0.1474<br>Epoch 20/250<br>313/313 - 1s - loss: 0.1396 - mse: 0.1396 - val_loss: 0.1479 - val_mse: 0.1479<br>Epoch 21/250<br>313/313 - 1s - loss: 0.1380 - mse: 0.1380 - val_loss: 0.1466 - val_mse: 0.1466<br>Epoch 22/250<br>313/313 - 0s - loss: 0.1366 - mse: 0.1366 - val_loss: 0.1438 - val_mse: 0.1438<br>Epoch 23/250<br>313/313 - 1s - loss: 0.1359 - mse: 0.1359 - val_loss: 0.1421 - val_mse: 0.1421<br>Epoch 24/250<br>313/313 - 1s - loss: 0.1352 - mse: 0.1352 - val_loss: 0.1595 - val_mse: 0.1595<br>Epoch 25/250<br>313/313 - 1s - loss: 0.1343 - mse: 0.1343 - val_loss: 0.1406 - val_mse: 0.1406<br>Epoch 26/250<br>313/313 - 1s - loss: 0.1332 - mse: 0.1332 - val_loss: 0.1408 - val_mse: 0.1408<br>Epoch 27/250<br>313/313 - 1s - loss: 0.1324 - mse: 0.1324 - val_loss: 0.1386 - val_mse: 0.1386<br>Epoch 28/250<br>313/313 - 1s - loss: 0.1314 - mse: 0.1314 - val_loss: 0.1412 - val_mse: 0.1412<br>Epoch 29/250<br>313/313 - 1s - loss: 0.1299 - mse: 0.1299 - val_loss: 0.1485 - val_mse: 0.1485<br>Epoch 30/250<br>313/313 - 0s - loss: 0.1301 - mse: 0.1301 - val_loss: 0.1373 - val_mse: 0.1373<br>Epoch 31/250<br>313/313 - 1s - loss: 0.1289 - mse: 0.1289 - val_loss: 0.1490 - val_mse: 0.1490<br>Epoch 32/250<br>313/313 - 0s - loss: 0.1275 - mse: 0.1275 - val_loss: 0.1362 - val_mse: 0.1362<br>Epoch 33/250<br>313/313 - 1s - loss: 0.1268 - mse: 0.1268 - val_loss: 0.1344 - val_mse: 0.1344<br>Epoch 34/250<br>313/313 - 0s - loss: 0.1254 - mse: 0.1254 - val_loss: 0.1454 - val_mse: 0.1454<br>Epoch 35/250<br>313/313 - 0s - loss: 0.1247 - mse: 0.1247 - val_loss: 0.1362 - val_mse: 0.1362<br>Epoch 36/250<br>313/313 - 1s - loss: 0.1247 - mse: 0.1247 - val_loss: 0.1321 - val_mse: 0.1321<br>Epoch 37/250<br>313/313 - 1s - loss: 0.1234 - mse: 0.1234 - val_loss: 0.1318 - val_mse: 0.1318<br>Epoch 38/250<br>313/313 - 1s - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1317 - val_mse: 0.1317<br>Epoch 39/250<br>313/313 - 1s - loss: 0.1215 - mse: 0.1215 - val_loss: 0.1313 - val_mse: 0.1313<br>Epoch 40/250<br>313/313 - 1s - loss: 0.1207 - mse: 0.1207 - val_loss: 0.1375 - val_mse: 0.1375<br>Epoch 41/250<br>313/313 - 1s - loss: 0.1197 - mse: 0.1197 - val_loss: 0.1386 - val_mse: 0.1386<br>Epoch 42/250<br>313/313 - 0s - loss: 0.1190 - mse: 0.1190 - val_loss: 0.1374 - val_mse: 0.1374<br>Epoch 43/250<br>313/313 - 1s - loss: 0.1181 - mse: 0.1181 - val_loss: 0.1280 - val_mse: 0.1280<br>Epoch 44/250<br>313/313 - 1s - loss: 0.1175 - mse: 0.1175 - val_loss: 0.1346 - val_mse: 0.1346<br>Epoch 45/250<br>313/313 - 1s - loss: 0.1167 - mse: 0.1167 - val_loss: 0.1286 - val_mse: 0.1286<br>Epoch 46/250<br>313/313 - 1s - loss: 0.1164 - mse: 0.1164 - val_loss: 0.1258 - val_mse: 0.1258<br>Epoch 47/250<br>313/313 - 0s - loss: 0.1153 - mse: 0.1153 - val_loss: 0.1304 - val_mse: 0.1304<br>Epoch 48/250<br>313/313 - 1s - loss: 0.1161 - mse: 0.1161 - val_loss: 0.1249 - val_mse: 0.1249<br>Epoch 49/250<br>313/313 - 0s - loss: 0.1148 - mse: 0.1148 - val_loss: 0.1249 - val_mse: 0.1249<br>Epoch 50/250<br>313/313 - 1s - loss: 0.1148 - mse: 0.1148 - val_loss: 0.1329 - val_mse: 0.1329<br>Epoch 51/250<br>313/313 - 0s - loss: 0.1141 - mse: 0.1141 - val_loss: 0.1256 - val_mse: 0.1256<br>Epoch 52/250<br>313/313 - 1s - loss: 0.1133 - mse: 0.1133 - val_loss: 0.1247 - val_mse: 0.1247<br>Epoch 53/250<br>313/313 - 0s - loss: 0.1130 - mse: 0.1130 - val_loss: 0.1284 - val_mse: 0.1284<br>Epoch 54/250<br>313/313 - 1s - loss: 0.1128 - mse: 0.1128 - val_loss: 0.1239 - val_mse: 0.1239<br>Epoch 55/250<br>313/313 - 1s - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1248 - val_mse: 0.1248<br>Epoch 56/250<br>313/313 - 1s - loss: 0.1116 - mse: 0.1116 - val_loss: 0.1234 - val_mse: 0.1234<br>Epoch 57/250<br>313/313 - 0s - loss: 0.1116 - mse: 0.1116 - val_loss: 0.1243 - val_mse: 0.1243<br>Epoch 58/250<br>313/313 - 0s - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1232 - val_mse: 0.1232<br>Epoch 59/250<br>313/313 - 0s - loss: 0.1107 - mse: 0.1107 - val_loss: 0.1248 - val_mse: 0.1248<br>Epoch 60/250<br>313/313 - 0s - loss: 0.1108 - mse: 0.1108 - val_loss: 0.1222 - val_mse: 0.1222<br>Epoch 61/250<br>313/313 - 1s - loss: 0.1101 - mse: 0.1101 - val_loss: 0.1221 - val_mse: 0.1221<br>Epoch 62/250<br>313/313 - 1s - loss: 0.1091 - mse: 0.1091 - val_loss: 0.1214 - val_mse: 0.1214<br>Epoch 63/250<br>313/313 - 1s - loss: 0.1099 - mse: 0.1099 - val_loss: 0.1242 - val_mse: 0.1242<br>Epoch 64/250<br>313/313 - 1s - loss: 0.1091 - mse: 0.1091 - val_loss: 0.1217 - val_mse: 0.1217<br>Epoch 65/250<br>313/313 - 1s - loss: 0.1090 - mse: 0.1090 - val_loss: 0.1253 - val_mse: 0.1253<br>Epoch 66/250<br>313/313 - 1s - loss: 0.1093 - mse: 0.1093 - val_loss: 0.1223 - val_mse: 0.1223<br>Epoch 67/250<br>313/313 - 1s - loss: 0.1082 - mse: 0.1082 - val_loss: 0.1267 - val_mse: 0.1267<br>Epoch 68/250<br>313/313 - 1s - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1228 - val_mse: 0.1228<br>Epoch 69/250<br>313/313 - 1s - loss: 0.1079 - mse: 0.1079 - val_loss: 0.1218 - val_mse: 0.1218<br>Epoch 70/250<br>313/313 - 1s - loss: 0.1078 - mse: 0.1078 - val_loss: 0.1205 - val_mse: 0.1205<br>Epoch 71/250<br>313/313 - 1s - loss: 0.1073 - mse: 0.1073 - val_loss: 0.1268 - val_mse: 0.1268<br>Epoch 72/250<br>313/313 - 1s - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1302 - val_mse: 0.1302<br>Epoch 73/250<br>313/313 - 1s - loss: 0.1065 - mse: 0.1065 - val_loss: 0.1209 - val_mse: 0.1209<br>Epoch 74/250<br>313/313 - 1s - loss: 0.1068 - mse: 0.1068 - val_loss: 0.1206 - val_mse: 0.1206<br>Epoch 75/250<br>313/313 - 0s - loss: 0.1062 - mse: 0.1062 - val_loss: 0.1206 - val_mse: 0.1206<br>Epoch 76/250<br>313/313 - 1s - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1202 - val_mse: 0.1202<br>Epoch 77/250<br>313/313 - 1s - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1349 - val_mse: 0.1349<br>Epoch 78/250<br>313/313 - 1s - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1240 - val_mse: 0.1240<br>Epoch 79/250<br>313/313 - 1s - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1333 - val_mse: 0.1333<br>Epoch 80/250<br>313/313 - 1s - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1198 - val_mse: 0.1198<br>Epoch 81/250<br>313/313 - 1s - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1192 - val_mse: 0.1192<br>Epoch 82/250<br>313/313 - 1s - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1199 - val_mse: 0.1199<br>Epoch 83/250<br>313/313 - 1s - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1216 - val_mse: 0.1216<br>Epoch 84/250<br>313/313 - 1s - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1225 - val_mse: 0.1225<br>Epoch 85/250<br>313/313 - 0s - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1293 - val_mse: 0.1293<br>Epoch 86/250<br>313/313 - 1s - loss: 0.1041 - mse: 0.1041 - val_loss: 0.1189 - val_mse: 0.1189<br>Epoch 87/250<br>313/313 - 1s - loss: 0.1035 - mse: 0.1035 - val_loss: 0.1218 - val_mse: 0.1218<br>Epoch 88/250<br>313/313 - 1s - loss: 0.1032 - mse: 0.1032 - val_loss: 0.1204 - val_mse: 0.1204<br>Epoch 89/250<br>313/313 - 1s - loss: 0.1037 - mse: 0.1037 - val_loss: 0.1210 - val_mse: 0.1210<br>Epoch 90/250<br>313/313 - 1s - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1197 - val_mse: 0.1197<br>Epoch 91/250<br>313/313 - 0s - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1219 - val_mse: 0.1219<br>Epoch 92/250<br>313/313 - 1s - loss: 0.1028 - mse: 0.1028 - val_loss: 0.1185 - val_mse: 0.1185<br>Epoch 93/250<br>313/313 - 0s - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1188 - val_mse: 0.1188<br>Epoch 94/250<br>313/313 - 1s - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1191 - val_mse: 0.1191<br>Epoch 95/250<br>313/313 - 1s - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1245 - val_mse: 0.1245<br>Epoch 96/250<br>313/313 - 1s - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1257 - val_mse: 0.1257<br>Epoch 97/250<br>313/313 - 0s - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1294 - val_mse: 0.1294<br>Epoch 98/250<br>313/313 - 1s - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1207 - val_mse: 0.1207<br>Epoch 99/250<br>313/313 - 0s - loss: 0.1008 - mse: 0.1008 - val_loss: 0.1189 - val_mse: 0.1189<br>Epoch 100/250<br>313/313 - 1s - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1211 - val_mse: 0.1211<br>Epoch 101/250<br>313/313 - 1s - loss: 0.1007 - mse: 0.1007 - val_loss: 0.1178 - val_mse: 0.1178<br>Epoch 102/250<br>313/313 - 1s - loss: 0.1010 - mse: 0.1010 - val_loss: 0.1246 - val_mse: 0.1246<br>Epoch 103/250<br>313/313 - 0s - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1252 - val_mse: 0.1252<br>Epoch 104/250<br>313/313 - 0s - loss: 0.1008 - mse: 0.1008 - val_loss: 0.1181 - val_mse: 0.1181<br>Epoch 105/250<br>313/313 - 1s - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1212 - val_mse: 0.1212<br>Epoch 106/250<br>313/313 - 0s - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1208 - val_mse: 0.1208<br>Epoch 107/250<br>313/313 - 0s - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1279 - val_mse: 0.1279<br>Epoch 108/250<br>313/313 - 1s - loss: 0.0997 - mse: 0.0997 - val_loss: 0.1183 - val_mse: 0.1183<br>Epoch 109/250<br>313/313 - 0s - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1179 - val_mse: 0.1179<br>Epoch 110/250<br>313/313 - 1s - loss: 0.0996 - mse: 0.0996 - val_loss: 0.1176 - val_mse: 0.1176<br>Epoch 111/250<br>313/313 - 0s - loss: 0.0993 - mse: 0.0993 - val_loss: 0.1180 - val_mse: 0.1180<br>Epoch 112/250<br>313/313 - 1s - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1176 - val_mse: 0.1176<br>Epoch 113/250<br>313/313 - 1s - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1284 - val_mse: 0.1284<br>Epoch 114/250<br>313/313 - 1s - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1180 - val_mse: 0.1180<br>Epoch 115/250<br>313/313 - 0s - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1226 - val_mse: 0.1226<br>Epoch 116/250<br>313/313 - 1s - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1180 - val_mse: 0.1180<br>Epoch 117/250<br>313/313 - 0s - loss: 0.0983 - mse: 0.0983 - val_loss: 0.1182 - val_mse: 0.1182<br>Epoch 118/250<br>313/313 - 1s - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1242 - val_mse: 0.1242<br>Epoch 119/250<br>313/313 - 0s - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1210 - val_mse: 0.1210<br>Epoch 120/250<br>313/313 - 1s - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1172 - val_mse: 0.1172<br>Epoch 121/250<br>313/313 - 1s - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1306 - val_mse: 0.1306<br>Epoch 122/250<br>313/313 - 1s - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1191 - val_mse: 0.1191<br>Epoch 123/250<br>313/313 - 0s - loss: 0.0976 - mse: 0.0976 - val_loss: 0.1173 - val_mse: 0.1173<br>Epoch 124/250<br>313/313 - 1s - loss: 0.0974 - mse: 0.0974 - val_loss: 0.1176 - val_mse: 0.1176<br>Epoch 125/250<br>313/313 - 1s - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1177 - val_mse: 0.1177<br>Epoch 126/250<br>313/313 - 0s - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1194 - val_mse: 0.1194        </sup></sub>    </details>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# imports\n",
    "from io import StringIO\n",
    "from IPython.core.display import display, HTML\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "## Capture standard output\n",
    "# standard output\n",
    "original_stdout = sys.stdout\n",
    "# replace standard out for intercepting logging\n",
    "sys.stdout = captured_stdout = StringIO()\n",
    "\n",
    "## Train Model\n",
    "# build model\n",
    "model = Sequential()\n",
    "# add new neurons\n",
    "model.add(Dense(50, input_dim = train_tf_input.shape[1]))\n",
    "model.add(Dense(25, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "# set optimizer for gradient descent\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['mse'])\n",
    "\n",
    "model.fit(\n",
    "    # training data\n",
    "    train_tf_input, train_tf_output,\n",
    "    # use test data to validate losses, but not for training\n",
    "    validation_data = (test_tf_input, test_tf_output),\n",
    "    callbacks = [\n",
    "        # patience: number of epochs with no improvement after which training will be stopped\n",
    "        EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 25, mode = 'auto', verbose = 0),\n",
    "        # save best model from all trials\n",
    "        ModelCheckpoint(filepath = \"temp/model_best_weights.hdf5\", save_best_only = True, verbose = 0)\n",
    "    ],\n",
    "    verbose = 2,\n",
    "    epochs = 250\n",
    ")\n",
    "\n",
    "## Logging\n",
    "# restore standard out\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "print(f\"Model trained in {time.time() - start_time:.6f} seconds\")\n",
    "# noinspection PyTypeChecker\n",
    "display(HTML(f'\\\n",
    "    <details>\\\n",
    "        <summary><b>Training Details</b></summary>\\\n",
    "        <sub><sup>\\\n",
    "            {\"<br>\".join(captured_stdout.getvalue().splitlines())}\\\n",
    "        </sup></sub>\\\n",
    "    </details>\\\n",
    "'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}