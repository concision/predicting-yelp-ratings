{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center\">Yelp Rating Prediction</h1>\n",
    "<hr style=\"border-top: 1px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Development Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# python typings\n",
    "from typing import TypedDict, Dict, List\n",
    "import json, time\n",
    "# libraries\n",
    "import sys, numpy, pandas, sklearn, tensorflow\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Numpy {numpy.__version__}\")\n",
    "print(f\"Pandas {pandas.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "print(f\"Tensor Flow Version: {tensorflow.__version__} (Keras Version: {tensorflow.keras.__version__})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]\n",
      "Numpy 1.18.5\n",
      "Pandas 1.1.1\n",
      "Scikit-Learn 0.23.2\n",
      "Tensor Flow Version: 2.3.0 (Keras Version: 2.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Data Importation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Location"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_businesses = r\"data/yelp_academic_dataset_business.json\"\n",
    "file_user_reviews = r\"data/yelp_academic_dataset_review.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Yelp Businesses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 209,393 distinct businesses in 3.802714 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# business structure\n",
    "class Business(TypedDict):\n",
    "    business_id: str\n",
    "    name: str\n",
    "    address: str\n",
    "    city: str\n",
    "    state: str\n",
    "    postal_code: str\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    stars: float\n",
    "    review_count: int\n",
    "    is_open: int\n",
    "    attributes: Dict\n",
    "    categories: List[str]\n",
    "    hours: Dict\n",
    "\n",
    "# businesses indexed by business_id (i.e. {business['business_id']: Business}\n",
    "businesses_by_id: Dict[str, Business] = {}\n",
    "\n",
    "# parse all businesses\n",
    "with open(file_businesses, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        business: Business = json.loads(record)\n",
    "        # map Business by business_id\n",
    "        businesses_by_id[business['business_id']] = business\n",
    "\n",
    "print(f\"Imported {len(businesses_by_id):,} distinct businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import User Reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 8,021,122 distinct businesses in 112.990777 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# user review structure\n",
    "class UserReview(TypedDict):\n",
    "    review_id: str\n",
    "    user_id: str\n",
    "    business_id: str\n",
    "    date: str\n",
    "    stars: int # [0, 1, 2, 3, 4, 5]\n",
    "    text: int\n",
    "    # review ratings\n",
    "    useful: int\n",
    "    funny: int\n",
    "    cool: int\n",
    "\n",
    "# user reviews indexed by business_id (i.e. {business_id: UserReview[]})\n",
    "business_reviews: Dict[str, List[UserReview]] = {\n",
    "    business_id: [] for business_id in businesses_by_id.keys()\n",
    "}\n",
    "\n",
    "# parse user reviews\n",
    "with open(file_user_reviews, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-deliminted JSON records\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        review: UserReview = json.loads(record)\n",
    "        # map user review by business_id\n",
    "        business_reviews[review['business_id']].append(review)\n",
    "\n",
    "print(f\"Imported {sum([len(reviews) for reviews in business_reviews.values()]):,} distinct businesses in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border-top: 1px solid #444\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select businesses with more than X reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103,803 remaining businesses (filtered 105590) in 0.090101 seconds\n"
     ]
    }
   ],
   "source": [
    "# minimum business[\"review_count\"] required for a business to not be filtered\n",
    "MINIMUM_REVIEW_COUNT = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# filter out businesses with less than MINIMUM_REVIEW_COUNT reviews\n",
    "filtered_businesses = [business for business in businesses_by_id.values() if MINIMUM_REVIEW_COUNT <= business[\"review_count\"]]\n",
    "\n",
    "print(f\"{len(filtered_businesses):,} remaining businesses (filtered {len(businesses_by_id) - len(filtered_businesses)}) in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partition businesses for model training and testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned 20,000 businesses into {training: 10000, testing: 10000} in 0.023006 seconds\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"represents the absolute number of test samples\"\n",
    "TRAINING_SIZE = 10000\n",
    "# \"represents the absolute number of train samples\"\n",
    "TESTING_SIZE = 10000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split businesses into two disjoint subsets: training and testing\n",
    "train_businesses, test_businesses = train_test_split(\n",
    "    # set to partition\n",
    "    filtered_businesses,\n",
    "    # partition proportions\n",
    "    train_size = TRAINING_SIZE,\n",
    "    test_size = TESTING_SIZE,\n",
    "    # shuffle the data\n",
    "    shuffle = True,\n",
    "    # PRNG seed for deterministic behaviour\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "print(f\"Partitioned {len(train_businesses) + len(test_businesses):,} businesses into {{training: {len(train_businesses)}, testing: {len(test_businesses)}}} in {time.time() - start_time:.6f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}